{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize smi\n",
    "def smi_tokenizer(smi):\n",
    "    \"\"\"\n",
    "    Tokenize a SMILES molecule or reaction\n",
    "    \"\"\"\n",
    "    import re\n",
    "    pattern =  \"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "    regex = re.compile(pattern)\n",
    "    tokens = [token for token in regex.findall(smi)]\n",
    "    assert smi == ''.join(tokens)\n",
    "    for i in range(len(tokens)):\n",
    "        if(tokens[i]=='[C@H]'):\n",
    "            tokens[i]='C'\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C C O C ( = O ) C c 1 c c c ( O C ) c ( O c 2 c c c ( [N+] ( = O ) [O-] ) c c 2 C Br ) c 1 . F C ( F ) ( F ) C S > > C O c 1 c c c ( C C ( = O ) O ) c c 1 O c 1 c c c ( [N+] ( = O ) [O-] ) c c 1 C Br'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer test\n",
    "smi=\"CCOC(=O)Cc1ccc(OC)c(Oc2ccc([N+](=O)[O-])cc2CBr)c1.FC(F)(F)CS>>COc1ccc(CC(=O)O)cc1Oc1ccc([N+](=O)[O-])cc1CBr\"\n",
    "smi_tokenizer(smi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rxn2rtpt(rxn_smi):\n",
    "    \"\"\"\n",
    "    Convert reaction SMILES into reactants and products\n",
    "    in a way that is compatible with `rdc.rdchiralRunText`\n",
    "    and not discarding meaningful data\n",
    "    \"\"\"\n",
    "    # for normal reactions\n",
    "    if rxn_smi.count('>>') == 1:\n",
    "        return rxn_smi.split('>>')\n",
    "\n",
    "    # for reactions with one reactant in middle\n",
    "    # like 'Cc1ccc(C(=O)O)cc1F>O=C1CCC(=O)N1Br>O=C(O)c1ccc(CBr)c(F)c1'\n",
    "    elif rxn_smi.count('>') == 2:\n",
    "        rt1, rt2, pt = rxn_smi.split('>')\n",
    "        # there are possible cases without rt1\n",
    "        # like '>O=C(O)C1CCN(C(=O)CO)CC1>O'\n",
    "        if rt1 == '':\n",
    "            return rt2, pt\n",
    "        else:\n",
    "            return f'{rt1}.{rt2}', pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read raw data: test\n",
    "data_dir=\"./data/All_Data/test.csv\"\n",
    "import pandas as pd\n",
    "data=pd.read_csv(data_dir)\n",
    "src_file=open(\"./data/All_Data/src_test_binary.txt\",\"w\")\n",
    "tgt_file=open(\"./data/All_Data/tgt_test_binary.txt\",\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate reactants and products\n",
    "for i in range(data.shape[0]):\n",
    "    src=data.loc[i][\"rxn_smiles\"]\n",
    "    tgt=data.loc[i][\"label\"]\n",
    "    src_token=smi_tokenizer(src)\n",
    "    src_file.write(src_token)\n",
    "    src_file.write(\"\\n\")\n",
    "    #tgt_token=smi_tokenizer(tgt)\n",
    "    tgt_file.write(str(tgt))\n",
    "    tgt_file.write(\"\\n\")\n",
    "src_file.close()\n",
    "tgt_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read raw data\n",
    "data_dir=\"./data/All_Data/train.csv\"\n",
    "import pandas as pd\n",
    "data=pd.read_csv(data_dir)\n",
    "src_file=open(\"./data/All_Data/src_train_binary.txt\",\"w\")\n",
    "tgt_file=open(\"./data/All_Data/tgt_train_binary.txt\",\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate reactants and products\n",
    "for i in range(data.shape[0]):\n",
    "    src=data.loc[i][\"rxn_smiles\"]\n",
    "    tgt=data.loc[i][\"label\"]\n",
    "    src_token=smi_tokenizer(src)\n",
    "    src_file.write(src_token)\n",
    "    src_file.write(\"\\n\")\n",
    "    #tgt_token=smi_tokenizer(tgt)\n",
    "    tgt_file.write(str(tgt))\n",
    "    tgt_file.write(\"\\n\")\n",
    "src_file.close()\n",
    "tgt_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read raw data\n",
    "data_dir=\"./data/All_Data/valid.csv\"\n",
    "import pandas as pd\n",
    "data=pd.read_csv(data_dir)\n",
    "src_file=open(\"./data/All_Data/src_val_binary.txt\",\"w\")\n",
    "tgt_file=open(\"./data/All_Data/tgt_val_binary.txt\",\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate reactants and products\n",
    "for i in range(data.shape[0]):\n",
    "    src=data.loc[i][\"rxn_smiles\"]\n",
    "    tgt=data.loc[i][\"label\"]\n",
    "    src_token=smi_tokenizer(src)\n",
    "    src_file.write(src_token)\n",
    "    src_file.write(\"\\n\")\n",
    "        #tgt_token=smi_tokenizer(tgt)\n",
    "    tgt_file.write(str(tgt))\n",
    "    tgt_file.write(\"\\n\")\n",
    "src_file.close()\n",
    "tgt_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-05-21 06:20:51,352 INFO] Extracting features...\n",
      "[2022-05-21 06:20:51,352 INFO]  * number of source features: 0.\n",
      "[2022-05-21 06:20:51,353 INFO]  * number of target features: 0.\n",
      "[2022-05-21 06:20:51,353 INFO] Building `Fields` object...\n",
      "[2022-05-21 06:20:51,353 INFO] Building & saving training data...\n",
      "[2022-05-21 06:20:51,353 INFO] Reading source and target files: data/All_Data/src_train_binary.txt data/All_Data/tgt_train_binary.txt.\n",
      "[2022-05-21 06:20:51,421 INFO] Splitting shard 0.\n",
      "[2022-05-21 06:20:51,468 INFO] Building shard 0.\n",
      "[2022-05-21 06:21:04,859 INFO]  * saving 0th train data shard to data/All_Data/All_Data.train.0.pt.\n",
      "[2022-05-21 06:21:10,282 INFO] Building & saving validation data...\n",
      "[2022-05-21 06:21:10,282 INFO] Reading source and target files: data/All_Data/src_val_binary.txt data/All_Data/tgt_val_binary.txt.\n",
      "[2022-05-21 06:21:10,288 INFO] Splitting shard 0.\n",
      "[2022-05-21 06:21:10,292 INFO] Building shard 0.\n",
      "[2022-05-21 06:21:12,130 INFO]  * saving 0th valid data shard to data/All_Data/All_Data.valid.0.pt.\n",
      "[2022-05-21 06:21:12,893 INFO] Building & saving vocabulary...\n",
      "[2022-05-21 06:21:13,514 INFO]  * reloading data/All_Data/All_Data.train.0.pt.\n",
      "[2022-05-21 06:21:14,670 INFO]  * tgt vocab size: 6.\n",
      "[2022-05-21 06:21:14,674 INFO]  * src vocab size: 1002.\n",
      "[2022-05-21 06:21:14,674 INFO]  * merging src and tgt vocab...\n"
     ]
    }
   ],
   "source": [
    "#input file generation\n",
    "!python preprocess.py -train_src data/All_Data/src_train_binary.txt \\\n",
    "                     -train_tgt data/All_Data/tgt_train_binary.txt \\\n",
    "                     -valid_src data/All_Data/src_val_binary.txt \\\n",
    "                     -valid_tgt data/All_Data/tgt_val_binary.txt \\\n",
    "                     -save_data data/All_Data/All_Data\\\n",
    "                     -src_seq_length 1000 -tgt_seq_length 1000 \\\n",
    "                     -src_vocab_size 1000 -tgt_vocab_size 1000 -share_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-05-21 06:21:16,538 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 06:21:16,540 INFO]  * vocabulary size. source = 1004; target = 1004\n",
      "[2022-05-21 06:21:16,540 INFO] Building model...\n",
      "[2022-05-21 06:21:21,013 INFO] NMTModel(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(1004, 256, padding_idx=1)\n",
      "        )\n",
      "        (pe): PositionalEncoding(\n",
      "          (dropout): Dropout(p=0.1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (transformer): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_values): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_query): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (layer_norm): LayerNorm()\n",
      "          (dropout_1): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "          (dropout_2): Dropout(p=0.1)\n",
      "        )\n",
      "        (layer_norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_values): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_query): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (layer_norm): LayerNorm()\n",
      "          (dropout_1): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "          (dropout_2): Dropout(p=0.1)\n",
      "        )\n",
      "        (layer_norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_values): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_query): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (layer_norm): LayerNorm()\n",
      "          (dropout_1): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "          (dropout_2): Dropout(p=0.1)\n",
      "        )\n",
      "        (layer_norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_values): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_query): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (layer_norm): LayerNorm()\n",
      "          (dropout_1): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "          (dropout_2): Dropout(p=0.1)\n",
      "        )\n",
      "        (layer_norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(1004, 256, padding_idx=1)\n",
      "        )\n",
      "        (pe): PositionalEncoding(\n",
      "          (dropout): Dropout(p=0.1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (transformer_layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_values): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_query): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_values): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_query): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (layer_norm): LayerNorm()\n",
      "          (dropout_1): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "          (dropout_2): Dropout(p=0.1)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm()\n",
      "        (layer_norm_2): LayerNorm()\n",
      "        (drop): Dropout(p=0.1)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_values): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_query): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_values): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_query): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (layer_norm): LayerNorm()\n",
      "          (dropout_1): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "          (dropout_2): Dropout(p=0.1)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm()\n",
      "        (layer_norm_2): LayerNorm()\n",
      "        (drop): Dropout(p=0.1)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_values): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_query): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_values): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_query): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (layer_norm): LayerNorm()\n",
      "          (dropout_1): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "          (dropout_2): Dropout(p=0.1)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm()\n",
      "        (layer_norm_2): LayerNorm()\n",
      "        (drop): Dropout(p=0.1)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_values): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_query): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_values): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_query): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (layer_norm): LayerNorm()\n",
      "          (dropout_1): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "          (dropout_2): Dropout(p=0.1)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm()\n",
      "        (layer_norm_2): LayerNorm()\n",
      "        (drop): Dropout(p=0.1)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=1004, bias=True)\n",
      "    (1): LogSoftmax()\n",
      "  )\n",
      ")\n",
      "[2022-05-21 06:21:21,018 INFO] encoder: 5517824\n",
      "[2022-05-21 06:21:21,019 INFO] decoder: 6573548\n",
      "[2022-05-21 06:21:21,019 INFO] * number of parameters: 12091372\n",
      "[2022-05-21 06:21:21,022 INFO] Start training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-05-21 06:21:21,759 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 06:22:49,036 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 06:24:14,956 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 06:25:38,701 INFO] Step 1000/15000; acc: 100.00; ppl:  1.00; xent: 0.00; lr: 0.00395; 58974/1490 tok/s;    257 sec\n",
      "[2022-05-21 06:25:38,704 INFO] Saving checkpoint experiments/checkpoints/All_Data_binary/All_Data_model_step_1000.pt\n",
      "[2022-05-21 06:25:40,538 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 06:27:06,478 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 06:28:32,490 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 06:29:57,473 INFO] Step 2000/15000; acc: 100.00; ppl:  1.00; xent: 0.00; lr: 0.00280; 59933/1045 tok/s;    516 sec\n",
      "[2022-05-21 06:29:57,476 INFO] Saving checkpoint experiments/checkpoints/All_Data_binary/All_Data_model_step_2000.pt\n",
      "[2022-05-21 06:29:59,677 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 06:31:25,848 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 06:32:51,742 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 06:34:15,847 INFO] Step 3000/15000; acc: 100.00; ppl:  1.00; xent: 0.00; lr: 0.00228; 62305/1111 tok/s;    774 sec\n",
      "[2022-05-21 06:34:15,850 INFO] Saving checkpoint experiments/checkpoints/All_Data_binary/All_Data_model_step_3000.pt\n",
      "[2022-05-21 06:34:18,575 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 06:35:47,212 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 06:37:13,378 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 06:38:37,268 INFO] Step 4000/15000; acc: 100.00; ppl:  1.00; xent: 0.00; lr: 0.00198; 65144/1309 tok/s;   1036 sec\n",
      "[2022-05-21 06:38:37,270 INFO] Saving checkpoint experiments/checkpoints/All_Data_binary/All_Data_model_step_4000.pt\n",
      "[2022-05-21 06:38:40,314 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 06:40:07,694 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 06:41:35,024 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 06:42:57,973 INFO] Step 5000/15000; acc: 100.00; ppl:  1.00; xent: 0.00; lr: 0.00177; 63718/1276 tok/s;   1296 sec\n",
      "[2022-05-21 06:42:57,975 INFO] Saving checkpoint experiments/checkpoints/All_Data_binary/All_Data_model_step_5000.pt\n",
      "[2022-05-21 06:43:02,752 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 06:44:29,224 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 06:45:56,523 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 06:47:19,664 INFO] Step 6000/15000; acc:  99.77; ppl:  1.02; xent: 0.02; lr: 0.00161; 67137/1867 tok/s;   1558 sec\n",
      "[2022-05-21 06:47:19,667 INFO] Saving checkpoint experiments/checkpoints/All_Data_binary/All_Data_model_step_6000.pt\n",
      "[2022-05-21 06:47:25,042 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 06:48:51,909 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 06:50:19,421 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 06:51:42,773 INFO] Step 7000/15000; acc: 100.00; ppl:  1.00; xent: 0.00; lr: 0.00149; 58448/1002 tok/s;   1821 sec\n",
      "[2022-05-21 06:51:42,776 INFO] Saving checkpoint experiments/checkpoints/All_Data_binary/All_Data_model_step_7000.pt\n",
      "[2022-05-21 06:51:48,789 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 06:53:14,129 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 06:54:40,780 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 06:56:02,451 INFO] Step 8000/15000; acc: 100.00; ppl:  1.00; xent: 0.00; lr: 0.00140; 58729/1064 tok/s;   2081 sec\n",
      "[2022-05-21 06:56:02,453 INFO] Saving checkpoint experiments/checkpoints/All_Data_binary/All_Data_model_step_8000.pt\n",
      "[2022-05-21 06:56:08,687 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 06:57:35,372 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 06:59:01,920 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 07:00:23,057 INFO] Step 9000/15000; acc: 100.00; ppl:  1.00; xent: 0.00; lr: 0.00132; 61046/1180 tok/s;   2341 sec\n",
      "[2022-05-21 07:00:23,059 INFO] Saving checkpoint experiments/checkpoints/All_Data_binary/All_Data_model_step_9000.pt\n",
      "[2022-05-21 07:00:29,942 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 07:01:55,389 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 07:03:22,390 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 07:04:43,408 INFO] Step 10000/15000; acc: 100.00; ppl:  1.00; xent: 0.00; lr: 0.00125; 61784/1429 tok/s;   2602 sec\n",
      "[2022-05-21 07:04:43,528 INFO] Loading valid dataset from data/All_Data/All_Data.valid.0.pt, number of examples: 8341\n",
      "[2022-05-21 07:04:48,442 INFO] Validation perplexity: 1\n",
      "[2022-05-21 07:04:48,443 INFO] Validation accuracy: 100\n",
      "[2022-05-21 07:04:48,444 INFO] Saving checkpoint experiments/checkpoints/All_Data_binary/All_Data_model_step_10000.pt\n",
      "[2022-05-21 07:04:54,853 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 07:06:22,135 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 07:07:48,779 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 07:09:08,032 INFO] Step 11000/15000; acc: 100.00; ppl:  1.00; xent: 0.00; lr: 0.00119; 63891/1418 tok/s;   2866 sec\n",
      "[2022-05-21 07:09:08,034 INFO] Saving checkpoint experiments/checkpoints/All_Data_binary/All_Data_model_step_11000.pt\n",
      "[2022-05-21 07:09:14,948 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 07:10:40,625 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 07:12:07,420 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 07:13:27,131 INFO] Step 12000/15000; acc: 100.00; ppl:  1.00; xent: 0.00; lr: 0.00114; 63468/1153 tok/s;   3125 sec\n",
      "[2022-05-21 07:13:27,133 INFO] Saving checkpoint experiments/checkpoints/All_Data_binary/All_Data_model_step_12000.pt\n",
      "[2022-05-21 07:13:34,164 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 07:14:59,993 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 07:16:26,606 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 07:17:45,991 INFO] Step 13000/15000; acc: 100.00; ppl:  1.00; xent: 0.00; lr: 0.00110; 63128/1214 tok/s;   3384 sec\n",
      "[2022-05-21 07:17:45,993 INFO] Saving checkpoint experiments/checkpoints/All_Data_binary/All_Data_model_step_13000.pt\n",
      "[2022-05-21 07:17:53,948 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 07:19:18,524 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-05-21 07:20:43,495 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 07:22:02,545 INFO] Step 14000/15000; acc: 100.00; ppl:  1.00; xent: 0.00; lr: 0.00106; 63179/1565 tok/s;   3641 sec\n",
      "[2022-05-21 07:22:02,547 INFO] Saving checkpoint experiments/checkpoints/All_Data_binary/All_Data_model_step_14000.pt\n",
      "[2022-05-21 07:22:10,720 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 07:23:37,549 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 07:25:03,752 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n",
      "[2022-05-21 07:26:22,244 INFO] Step 15000/15000; acc: 100.00; ppl:  1.00; xent: 0.00; lr: 0.00102; 62748/1535 tok/s;   3900 sec\n",
      "[2022-05-21 07:26:22,247 INFO] Saving checkpoint experiments/checkpoints/All_Data_binary/All_Data_model_step_15000.pt\n",
      "[2022-05-21 07:26:23,425 INFO] Loading train dataset from data/All_Data/All_Data.train.0.pt, number of examples: 58518\n"
     ]
    }
   ],
   "source": [
    "#train a model\n",
    "!python  train.py -data data/All_Data/All_Data \\\n",
    "                   -save_model experiments/checkpoints/All_Data_binary/All_Data_model \\\n",
    "                   -seed 42 -gpu_ranks 0 -save_checkpoint_steps 1000 -keep_checkpoint 5 \\\n",
    "                   -train_steps 15000 -param_init 0  -param_init_glorot -max_generator_batches 32 \\\n",
    "                   -batch_size 4096 -batch_type tokens -normalization tokens -max_grad_norm 0  -accum_count 4 \\\n",
    "                   -optim adam -adam_beta1 0.9 -adam_beta2 0.998 -decay_method noam -warmup_steps 1000  \\\n",
    "                   -learning_rate 2 -label_smoothing 0.0 -report_every 1000 \\\n",
    "                   -layers 4 -rnn_size 256 -word_vec_size 256 -encoder_type transformer -decoder_type transformer \\\n",
    "                   -dropout 0.1 -position_encoding -share_embeddings \\\n",
    "                   -global_attention general -global_attention_function softmax -self_attn_type scaled-dot \\\n",
    "                   -heads 8 -transformer_ff 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED AVG SCORE: -0.0000, PRED PPL: 1.0000\r\n"
     ]
    }
   ],
   "source": [
    "#make predictions\n",
    "!python translate.py -model ./experiments/checkpoints/All_Data_binary/All_Data_model_step_15000.pt -src ./data/All_Data/src_test_binary.txt -output ./experiments/results/All_Data_pred.txt -n_best 1 -batch_size 128 -gpu 0 -replace_unk -max_length 5 -fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the acc\n",
    "data_dir=\"./data/All_Data/test.csv\"\n",
    "import pandas as pd\n",
    "data=pd.read_csv(data_dir)\n",
    "\n",
    "pred_text=open(\"./experiments/results/All_Data_pred.txt\")\n",
    "lines=pred_text.readlines()\n",
    "count=len(data)\n",
    "false=0\n",
    "for i in range(len(data)):\n",
    "    gt_product=data.loc[i][\"label\"]\n",
    "    pred=lines[i].strip(\"\\n\")\n",
    "    if(int(pred)!=gt_product):\n",
    "        false+=1 \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false/count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
